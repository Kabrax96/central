# central
Automatizar informaciÃ³n de consejo Nuevo LeÃ³n



# ğŸ“Š Presupuesto Data ETL Pipeline

## ğŸ† **Project Overview**
This ETL (Extract, Transform, Load) pipeline automates the process of extracting presupuesto data from multiple **CSV** and **Excel** files, transforming it into a structured format, and loading it into a **PostgreSQL** database. The pipeline handles both individual and bulk file processing while maintaining detailed logs for auditing and troubleshooting.

---

## ğŸ“ **Project Structure**

```
etl_project/
â”‚
â”œâ”€â”€ assets/                           # Core ETL functions and utilities
â”‚   â”œâ”€â”€ presupuesto_etl.py            # Extract, Transform, Load functions
â”‚   â””â”€â”€ pipeline_logging.py           # Logging utility (Loguru-based)
â”‚
â”œâ”€â”€ connectors/                       # Database connectors
â”‚   â””â”€â”€ postgresql.py                 # PostgreSQL client connector
â”‚
â”œâ”€â”€ pipelines/                        # ETL orchestrators
â”‚   â”œâ”€â”€ presupuesto_pipeline.py       # ETL for single file
â”‚   â””â”€â”€ bulk_presupuesto_pipeline.py  # ETL for processing multiple files
â”‚
â”œâ”€â”€ data/                             # Raw data files
â”‚   â””â”€â”€ financial_files/              # Contains all CSV/Excel files
â”‚
â”œâ”€â”€ logs/                             # Generated logs from ETL process
â”‚   â””â”€â”€ pipeline_logging<timestamp>.log
â”‚
â”œâ”€â”€ .env                              # Environment variables (DB credentials)
â”‚
â””â”€â”€ requirements.txt                  # Project dependencies
```

---

## âš™ï¸ **Setup & Installation**

### 1ï¸âƒ£ **Clone the Repository:**
```bash
git clone <repo_url>
cd central
```

### 2ï¸âƒ£ **Create & Activate a Virtual Environment:**
```bash
python -m venv venv
source venv/bin/activate   # On Windows: venv\Scripts\activate
```

### 3ï¸âƒ£ **Install Dependencies:**
```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ **Configure Environment Variables (.env):**
Create a `.env` file in the project root with the following content:

```env
DB_USERNAME=your_db_username
DB_PASSWORD=your_db_password
SERVER_NAME=localhost
DATABASE_NAME=your_database_name
PORT=5432
```

### 5ï¸âƒ£ **Verify PostgreSQL is Running:**
Ensure that PostgreSQL is running locally or remotely and is accessible with the credentials provided in the `.env` file.

---

## ğŸ“‚ **Processing Presupuesto Data**

### âœ… **Run ETL for a Single File:**
To run the ETL process for a single file:

```bash
python central/pipelines/presupuesto_pipeline.py
```

### âœ… **Run ETL for Multiple Files:**
To run the ETL process for all files in the `data/presupuesto_files/` directory:

```bash
python central/pipelines/bulk_presupuesto_pipeline.py
```

The pipeline will process all `.csv` and `.xlsx` files in the directory.

---

## ğŸ“Š **Logging System**

The pipeline uses **Loguru** for advanced logging. Logs include timestamps, log levels, status codes, and descriptive messages.

### âœ… **Log Storage:**
- Logs are stored in the `logs/` directory.
- Each pipeline run creates a **new log file** with a timestamp in its name.

### âœ… **Log Codes:**
| **Code** | **Meaning**                    |
|----------|--------------------------------|
| **100**  | Process started or in progress |
| **200**  | Success/Completion             |
| **400**  | Warning/Recoverable issue      |
| **500**  | Critical Error/Failure         |

---

## ğŸ§® **ETL Process Flow**

1. **Extract** â€” Reads data from CSV/Excel files.
2. **Transform** â€”
    - Cleans and formats the data.
    - Extracts metadata (e.g., year, quarter).
    - Prepares data for insertion into PostgreSQL.
3. **Load** â€”
    - Inserts or upserts data into the PostgreSQL database.
    - Handles conflicts using PostgreSQL's `ON CONFLICT` resolution.

---

## ğŸ’¾ **PostgreSQL Table Structure:**

The financial data is loaded into the `nuevo_leon_financials` table with the following schema:

| **Column**      | **Type** |
|-----------------|----------|
| concept         | String   |
| sublabel        | String   |
| year_quarter    | String   |
| type            | String   |
| amount          | Float    |

---

## ğŸ’¡ **Additional Notes:**

- **Error Handling:**
  - The pipeline logs all errors and skips problematic files during bulk operations.
  - Errors are logged with a **500** status code.

- **Performance:**
  - Log rotation set to **500 MB**.
  - Logs retained for **10 days**.

- **Scalability:**
  - The pipeline is designed to handle large datasets and multiple files.

---

## ğŸ¤ **Contributing:**

1. Fork the repository.
2. Create a new branch (`feature/your-feature`).
3. Commit your changes.
4. Push to the branch.
5. Create a Pull Request.

---

## ğŸ“¬ **Questions & Support:**

If you have any questions or run into issues, please contact the project maintainer or open an issue in the repository.
amoralesb196@gmail.com

Happy ETLing! ğŸš€
